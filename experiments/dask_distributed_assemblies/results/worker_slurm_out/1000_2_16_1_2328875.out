2024-08-09 04:13:07,662 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:45673'
2024-08-09 04:13:07,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:41188'
2024-08-09 04:13:07,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:39747'
2024-08-09 04:13:07,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:37450'
2024-08-09 04:13:07,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:46491'
2024-08-09 04:13:07,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:36232'
2024-08-09 04:13:07,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:37752'
2024-08-09 04:13:07,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:42988'
2024-08-09 04:13:07,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:43838'
2024-08-09 04:13:07,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:34996'
2024-08-09 04:13:07,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:35608'
2024-08-09 04:13:07,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:43477'
2024-08-09 04:13:07,690 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:43620'
2024-08-09 04:13:07,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:36379'
2024-08-09 04:13:07,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:46406'
2024-08-09 04:13:07,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:36853'
2024-08-09 04:13:08,284 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-lxwi272w', purging
2024-08-09 04:13:08,284 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_ywzhwd9', purging
2024-08-09 04:13:08,285 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fy1lbmcz', purging
2024-08-09 04:13:08,285 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ii2hy_gc', purging
2024-08-09 04:13:08,285 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vvt4m6it', purging
2024-08-09 04:13:08,285 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-iwg3qeko', purging
2024-08-09 04:13:08,285 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-oraibdxh', purging
2024-08-09 04:13:08,286 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3hq0gv3s', purging
2024-08-09 04:13:08,803 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:35845
2024-08-09 04:13:08,803 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:35845
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2024-08-09 04:13:08,803 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:35627
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:44520
2024-08-09 04:13:08,803 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:46240
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:42051
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:38791
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:35627
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:35846
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:46240
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-4
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:42051
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-7
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:36781
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:38791
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:35846
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:41525
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:43241
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-5
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:36781
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:43406
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2g6m0m53
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:46467
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:36440
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-6
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:43547
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o8asw0ts
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-096mjdq_
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ht5em638
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q2s9lmqm
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-my04m9pl
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p30wxpxw
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,804 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:41153
2024-08-09 04:13:08,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:41153
2024-08-09 04:13:08,804 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2024-08-09 04:13:08,804 - distributed.worker - INFO -          dashboard at:           10.34.59.4:33961
2024-08-09 04:13:08,804 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,804 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,805 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,805 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,805 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3g805kt3
2024-08-09 04:13:08,805 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,805 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:38852
2024-08-09 04:13:08,805 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:41899
2024-08-09 04:13:08,805 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:38852
2024-08-09 04:13:08,805 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-8
2024-08-09 04:13:08,805 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:41899
2024-08-09 04:13:08,805 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:37775
2024-08-09 04:13:08,805 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:40586
2024-08-09 04:13:08,806 - distributed.worker - INFO -          dashboard at:           10.34.59.4:33287
2024-08-09 04:13:08,806 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-9
2024-08-09 04:13:08,807 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:37775
2024-08-09 04:13:08,807 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:40586
2024-08-09 04:13:08,807 - distributed.worker - INFO -          dashboard at:           10.34.59.4:44024
2024-08-09 04:13:08,807 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,807 - distributed.worker - INFO -           Worker name:          SLURMCluster-1-10
2024-08-09 04:13:08,808 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,808 - distributed.worker - INFO -           Worker name:          SLURMCluster-1-11
2024-08-09 04:13:08,808 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,808 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,808 - distributed.worker - INFO -          dashboard at:           10.34.59.4:42188
2024-08-09 04:13:08,808 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,808 - distributed.worker - INFO -          dashboard at:           10.34.59.4:40500
2024-08-09 04:13:08,808 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,808 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,808 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,808 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,808 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,808 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:37023
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rm_wxxgl
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-86tab7fm
2024-08-09 04:13:08,809 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,809 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:37023
2024-08-09 04:13:08,809 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,809 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5rovid0y
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m66dx9zk
2024-08-09 04:13:08,809 - distributed.worker - INFO -           Worker name:          SLURMCluster-1-14
2024-08-09 04:13:08,809 - distributed.worker - INFO -          dashboard at:           10.34.59.4:35033
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,809 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zahkrciu
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:35370
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:35370
2024-08-09 04:13:08,809 - distributed.worker - INFO -           Worker name:          SLURMCluster-1-13
2024-08-09 04:13:08,809 - distributed.worker - INFO -          dashboard at:           10.34.59.4:46196
2024-08-09 04:13:08,809 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,809 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,809 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-09mneld7
2024-08-09 04:13:08,809 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,811 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:40012
2024-08-09 04:13:08,811 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:40012
2024-08-09 04:13:08,811 - distributed.worker - INFO -           Worker name:          SLURMCluster-1-12
2024-08-09 04:13:08,812 - distributed.worker - INFO -          dashboard at:           10.34.59.4:44938
2024-08-09 04:13:08,812 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,812 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,812 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,812 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,812 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qegoc3q5
2024-08-09 04:13:08,812 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,818 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:35905
2024-08-09 04:13:08,818 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:35905
2024-08-09 04:13:08,819 - distributed.worker - INFO -           Worker name:          SLURMCluster-1-15
2024-08-09 04:13:08,819 - distributed.worker - INFO -          dashboard at:           10.34.59.4:38677
2024-08-09 04:13:08,819 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45994
2024-08-09 04:13:08,819 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:08,819 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:13:08,819 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:13:08,819 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vjj2_ppl
2024-08-09 04:13:08,819 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,192 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,193 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,193 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,193 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,193 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,194 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,194 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,194 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,195 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,195 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,195 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,196 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,196 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,196 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,196 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,197 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,197 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,198 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,198 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,198 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,198 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,198 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,199 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,199 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,199 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,200 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,200 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,200 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,201 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,201 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,201 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,212 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,212 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,213 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,213 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,213 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,214 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,214 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,214 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,214 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,215 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,215 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,215 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,216 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,216 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,216 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
2024-08-09 04:13:09,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:13:09,232 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45994
2024-08-09 04:13:09,232 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:13:09,232 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45994
slurmstepd: error: *** JOB 2328875 ON cl-node004 CANCELLED AT 2024-08-09T04:13:17 ***
2024-08-09 04:13:17,213 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:35627. Reason: scheduler-close
2024-08-09 04:13:17,213 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:40012. Reason: scheduler-close
2024-08-09 04:13:17,213 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:35370. Reason: scheduler-close
2024-08-09 04:13:17,214 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:41153. Reason: scheduler-close
2024-08-09 04:13:17,214 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:40586. Reason: scheduler-close
2024-08-09 04:13:17,214 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:37775. Reason: scheduler-close
2024-08-09 04:13:17,214 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:42051. Reason: scheduler-close
2024-08-09 04:13:17,214 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:38852. Reason: scheduler-close
2024-08-09 04:13:17,214 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:35845. Reason: scheduler-close
2024-08-09 04:13:17,215 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:46240. Reason: scheduler-close
2024-08-09 04:13:17,215 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:41899. Reason: scheduler-close
2024-08-09 04:13:17,215 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:36781. Reason: scheduler-close
2024-08-09 04:13:17,215 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:35846. Reason: scheduler-close
2024-08-09 04:13:17,215 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:37023. Reason: scheduler-close
2024-08-09 04:13:17,216 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:38791. Reason: scheduler-close
2024-08-09 04:13:17,215 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:35905. Reason: scheduler-close
2024-08-09 04:13:17,218 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:57314 remote=tcp://10.34.59.1:45994>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:57314 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,218 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:57316 remote=tcp://10.34.59.1:45994>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:57316 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,218 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:57312 remote=tcp://10.34.59.1:45994>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:57312 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,219 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:46491'. Reason: scheduler-close
2024-08-09 04:13:17,222 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,222 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,223 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:43620'. Reason: scheduler-close
2024-08-09 04:13:17,224 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:39747'. Reason: scheduler-close
2024-08-09 04:13:17,224 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:36379'. Reason: scheduler-close
2024-08-09 04:13:17,224 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:43477'. Reason: scheduler-close
2024-08-09 04:13:17,225 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:35608'. Reason: scheduler-close
2024-08-09 04:13:17,225 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:41188'. Reason: scheduler-close
2024-08-09 04:13:17,225 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,225 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:37450'. Reason: scheduler-close
2024-08-09 04:13:17,226 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,226 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,226 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,226 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:43838'. Reason: scheduler-close
2024-08-09 04:13:17,227 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,227 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,227 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,227 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,227 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,227 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,227 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,227 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,228 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,228 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,228 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,228 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,229 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:34996'. Reason: scheduler-close
2024-08-09 04:13:17,230 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:42988'. Reason: scheduler-close
2024-08-09 04:13:17,230 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:46406'. Reason: scheduler-close
2024-08-09 04:13:17,230 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:36853'. Reason: scheduler-close
2024-08-09 04:13:17,231 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:36232'. Reason: scheduler-close
2024-08-09 04:13:17,231 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:45673'. Reason: scheduler-close
2024-08-09 04:13:17,231 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,231 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:37752'. Reason: scheduler-close
2024-08-09 04:13:17,232 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,231 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.4:57360 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,231 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.4:57362 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.4:57356 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.4:57358 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,234 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,234 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,234 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,232 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.4:57354 remote=tcp://10.34.59.1:45994>: Stream is closed
2024-08-09 04:13:17,234 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,234 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,234 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,234 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,234 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,234 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,234 - distributed.nanny - INFO - Worker closed
2024-08-09 04:13:17,235 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:45994; closing.
2024-08-09 04:13:17,235 - distributed.nanny - INFO - Worker closed
