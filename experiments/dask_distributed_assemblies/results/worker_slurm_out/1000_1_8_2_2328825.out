2024-08-09 03:59:36,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43737'
2024-08-09 03:59:36,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37474'
2024-08-09 03:59:36,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44919'
2024-08-09 03:59:36,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36937'
2024-08-09 03:59:36,929 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44944'
2024-08-09 03:59:36,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46207'
2024-08-09 03:59:36,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37773'
2024-08-09 03:59:36,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37210'
2024-08-09 03:59:38,073 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41930
2024-08-09 03:59:38,073 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37888
2024-08-09 03:59:38,073 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41930
2024-08-09 03:59:38,073 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37888
2024-08-09 03:59:38,073 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36959
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 03:59:38,073 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36294
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41328
2024-08-09 03:59:38,074 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36959
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35307
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36294
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35078
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43517
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40337
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43517
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46290
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pj2nc5e2
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42422
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kmrh5j3w
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46290
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35488
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33944
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42422
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q0pv4fy7
2024-08-09 03:59:38,074 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35488
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46134
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_e4o164p
2024-08-09 03:59:38,074 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40463
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45769
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-alr3e7gk
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO -               Threads:                          1
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-07t6hthg
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ecoufjbf
2024-08-09 03:59:38,074 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gk3ofl1n
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,074 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,456 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,456 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,456 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,456 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,457 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,457 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,457 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,457 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,458 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,458 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,458 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,458 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,459 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,459 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,459 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,460 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,459 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,460 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,460 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,461 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,461 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,461 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,469 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,470 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,470 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:38,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 03:59:38,471 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34598
2024-08-09 03:59:38,471 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 03:59:38,471 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34598
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37888. Reason: scheduler-close
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35488. Reason: scheduler-close
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36959. Reason: scheduler-close
slurmstepd: error: *** JOB 2328825 ON cl-node002 CANCELLED AT 2024-08-09T03:59:47 ***
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46290. Reason: scheduler-close
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41930. Reason: scheduler-close
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36294. Reason: scheduler-close
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43517. Reason: scheduler-close
2024-08-09 03:59:47,317 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42422. Reason: scheduler-close
2024-08-09 03:59:47,318 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48820 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48820 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,318 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48818 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48818 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,318 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48830 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48830 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,318 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48828 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48828 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,319 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48822 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48822 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,319 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48816 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48816 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,320 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48826 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48826 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,320 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48824 remote=tcp://10.34.59.1:34598>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48824 remote=tcp://10.34.59.1:34598>: Stream is closed
2024-08-09 03:59:47,322 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44944'. Reason: scheduler-close
2024-08-09 03:59:47,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36937'. Reason: scheduler-close
2024-08-09 03:59:47,323 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43737'. Reason: scheduler-close
2024-08-09 03:59:47,324 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,325 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,325 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,325 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46207'. Reason: scheduler-close
2024-08-09 03:59:47,325 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44919'. Reason: scheduler-close
2024-08-09 03:59:47,325 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,325 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37773'. Reason: scheduler-close
2024-08-09 03:59:47,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37210'. Reason: scheduler-close
2024-08-09 03:59:47,326 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37474'. Reason: scheduler-close
2024-08-09 03:59:47,328 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,328 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,329 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,329 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,329 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,329 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,329 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,329 - distributed.nanny - INFO - Worker closed
2024-08-09 03:59:47,329 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34598; closing.
2024-08-09 03:59:47,329 - distributed.nanny - INFO - Worker closed
