2024-08-09 05:58:13,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41019'
2024-08-09 05:58:13,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35808'
2024-08-09 05:58:13,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44002'
2024-08-09 05:58:13,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46248'
2024-08-09 05:58:13,668 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46880'
2024-08-09 05:58:13,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46203'
2024-08-09 05:58:13,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34258'
2024-08-09 05:58:13,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39830'
2024-08-09 05:58:13,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33614'
2024-08-09 05:58:13,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44530'
2024-08-09 05:58:13,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39657'
2024-08-09 05:58:13,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33787'
2024-08-09 05:58:13,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44149'
2024-08-09 05:58:13,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39661'
2024-08-09 05:58:13,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34151'
2024-08-09 05:58:13,686 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44834'
2024-08-09 05:58:14,775 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38044
2024-08-09 05:58:14,775 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38044
2024-08-09 05:58:14,775 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 05:58:14,775 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39615
2024-08-09 05:58:14,775 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,775 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,775 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,775 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,775 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o58i47rn
2024-08-09 05:58:14,775 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,777 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34007
2024-08-09 05:58:14,777 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34007
2024-08-09 05:58:14,777 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 05:58:14,777 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37695
2024-08-09 05:58:14,777 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,778 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,778 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u29qrnc4
2024-08-09 05:58:14,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,779 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42695
2024-08-09 05:58:14,779 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42695
2024-08-09 05:58:14,779 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 05:58:14,779 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45497
2024-08-09 05:58:14,779 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40662
2024-08-09 05:58:14,779 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,779 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40662
2024-08-09 05:58:14,780 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,780 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 05:58:14,780 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,780 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41938
2024-08-09 05:58:14,780 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6texoc9a
2024-08-09 05:58:14,780 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,780 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,780 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,780 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,780 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,780 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-icgrbm05
2024-08-09 05:58:14,780 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,781 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46643
2024-08-09 05:58:14,782 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46643
2024-08-09 05:58:14,782 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 05:58:14,782 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35069
2024-08-09 05:58:14,782 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,782 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sgzk1bb5
2024-08-09 05:58:14,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,782 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33782
2024-08-09 05:58:14,782 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33782
2024-08-09 05:58:14,782 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 05:58:14,782 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33963
2024-08-09 05:58:14,782 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,783 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,783 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-js3gitvo
2024-08-09 05:58:14,783 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,790 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45542
2024-08-09 05:58:14,790 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45542
2024-08-09 05:58:14,790 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 05:58:14,790 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33129
2024-08-09 05:58:14,790 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,790 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,790 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,790 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q_z35j8g
2024-08-09 05:58:14,790 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,791 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43283
2024-08-09 05:58:14,792 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43283
2024-08-09 05:58:14,792 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 05:58:14,792 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34937
2024-08-09 05:58:14,792 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,792 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,792 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,792 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j0262uh0
2024-08-09 05:58:14,792 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,795 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45669
2024-08-09 05:58:14,795 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45669
2024-08-09 05:58:14,795 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 05:58:14,795 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39346
2024-08-09 05:58:14,795 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,795 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,795 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,795 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ct2w_z_v
2024-08-09 05:58:14,796 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,797 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45357
2024-08-09 05:58:14,797 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45357
2024-08-09 05:58:14,798 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 05:58:14,798 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43761
2024-08-09 05:58:14,798 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,798 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,798 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,798 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m__wasy6
2024-08-09 05:58:14,798 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,799 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33523
2024-08-09 05:58:14,799 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33523
2024-08-09 05:58:14,799 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 05:58:14,799 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45453
2024-08-09 05:58:14,799 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,799 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,799 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,799 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,799 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k0h6sbc1
2024-08-09 05:58:14,799 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,801 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40337
2024-08-09 05:58:14,801 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35068
2024-08-09 05:58:14,801 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35068
2024-08-09 05:58:14,801 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40337
2024-08-09 05:58:14,801 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 05:58:14,801 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 05:58:14,801 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40332
2024-08-09 05:58:14,801 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44672
2024-08-09 05:58:14,801 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,801 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,801 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,801 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,801 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,801 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,801 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,801 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qs7bt5lf
2024-08-09 05:58:14,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c0znvw3c
2024-08-09 05:58:14,801 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,801 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,801 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35176
2024-08-09 05:58:14,802 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35176
2024-08-09 05:58:14,802 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 05:58:14,802 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42608
2024-08-09 05:58:14,802 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,802 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,802 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,802 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ugw986ny
2024-08-09 05:58:14,803 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,804 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40851
2024-08-09 05:58:14,804 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40851
2024-08-09 05:58:14,805 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 05:58:14,805 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45928
2024-08-09 05:58:14,805 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,805 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,805 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,805 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,805 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l1k0lih0
2024-08-09 05:58:14,805 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33048
2024-08-09 05:58:14,805 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,805 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33048
2024-08-09 05:58:14,805 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 05:58:14,806 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33083
2024-08-09 05:58:14,806 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:45775
2024-08-09 05:58:14,806 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:14,806 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:58:14,806 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:58:14,806 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i0cdqj54
2024-08-09 05:58:14,806 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,156 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,156 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,156 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,157 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,158 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,158 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,158 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,159 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,159 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,159 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,159 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,160 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,162 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,163 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,163 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,163 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,166 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,166 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,167 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,167 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,167 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,167 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,169 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,169 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,170 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,174 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,174 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,174 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,177 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,178 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,178 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,178 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,179 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,179 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,180 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,184 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,184 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,184 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,193 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,194 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,194 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,200 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,200 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,200 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,202 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,202 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:58:15,203 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:15,203 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:45775
2024-08-09 05:58:15,203 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:58:15,204 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:45775
2024-08-09 05:58:18,357 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 1.63 GiB -- Worker memory limit: 1.86 GiB
2024-08-09 05:58:18,380 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.63 GiB -- Worker memory limit: 1.86 GiB
2024-08-09 05:58:18,475 - distributed.worker.memory - WARNING - Worker is at 37% memory usage. Resuming worker. Process memory: 715.68 MiB -- Worker memory limit: 1.86 GiB
slurmstepd: error: *** JOB 2330383 ON cl-node002 CANCELLED AT 2024-08-09T05:58:26 ***
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34007. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42695. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38044. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40662. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45542. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46643. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43283. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40337. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35176. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33048. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40851. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45669. Reason: scheduler-close
2024-08-09 05:58:26,990 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33782. Reason: scheduler-close
2024-08-09 05:58:26,992 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33523. Reason: scheduler-close
2024-08-09 05:58:26,992 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35068. Reason: scheduler-close
2024-08-09 05:58:26,992 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45357. Reason: scheduler-close
2024-08-09 05:58:26,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52532 remote=tcp://10.34.59.1:45775>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52532 remote=tcp://10.34.59.1:45775>: Stream is closed
2024-08-09 05:58:26,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52538 remote=tcp://10.34.59.1:45775>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52538 remote=tcp://10.34.59.1:45775>: Stream is closed
2024-08-09 05:58:26,993 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52542 remote=tcp://10.34.59.1:45775>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52542 remote=tcp://10.34.59.1:45775>: Stream is closed
2024-08-09 05:58:26,994 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52544 remote=tcp://10.34.59.1:45775>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52544 remote=tcp://10.34.59.1:45775>: Stream is closed
2024-08-09 05:58:26,994 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52540 remote=tcp://10.34.59.1:45775>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52540 remote=tcp://10.34.59.1:45775>: Stream is closed
