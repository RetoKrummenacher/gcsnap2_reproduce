2024-08-09 07:47:30,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:34374'
2024-08-09 07:47:30,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:33401'
2024-08-09 07:47:30,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:33003'
2024-08-09 07:47:30,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:37095'
2024-08-09 07:47:30,171 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:40506'
2024-08-09 07:47:30,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:40007'
2024-08-09 07:47:30,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:37705'
2024-08-09 07:47:30,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.4:44285'
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:38031
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:46862
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:36319
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:35917
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:38031
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:46862
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:36319
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:35917
2024-08-09 07:47:31,600 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-5
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:43579
2024-08-09 07:47:31,600 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-4
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:37727
2024-08-09 07:47:31,600 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-6
2024-08-09 07:47:31,600 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:37606
2024-08-09 07:47:31,600 - distributed.worker - INFO -          dashboard at:           10.34.59.4:33762
2024-08-09 07:47:31,600 - distributed.worker - INFO -          dashboard at:           10.34.59.4:34503
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:43579
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:37727
2024-08-09 07:47:31,600 - distributed.worker - INFO -          dashboard at:           10.34.59.4:34206
2024-08-09 07:47:31,600 - distributed.worker - INFO -          dashboard at:           10.34.59.4:39928
2024-08-09 07:47:31,600 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.4:39265
2024-08-09 07:47:31,600 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,600 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,600 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:37606
2024-08-09 07:47:31,600 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2024-08-09 07:47:31,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2024-08-09 07:47:31,601 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,601 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.4:39265
2024-08-09 07:47:31,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -          dashboard at:           10.34.59.4:34147
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO -          dashboard at:           10.34.59.4:35306
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-7
2024-08-09 07:47:31,601 - distributed.worker - INFO -          dashboard at:           10.34.59.4:45867
2024-08-09 07:47:31,601 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9lgteq53
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -          dashboard at:           10.34.59.4:38182
2024-08-09 07:47:31,601 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ksmjnt6q
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0htsh7pk
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-na01dtnp
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43219
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yyx0iuqq
2024-08-09 07:47:31,601 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wurkekxa
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tnkmcwmo
2024-08-09 07:47:31,601 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hrnjuwk0
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:31,601 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,029 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,030 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,030 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,030 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,030 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,031 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,031 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,031 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,032 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,032 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,032 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,032 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,033 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,033 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,033 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,033 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,033 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,034 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,034 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,034 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,034 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,035 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,036 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,036 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,036 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:32,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:47:32,037 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43219
2024-08-09 07:47:32,037 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:47:32,037 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43219
2024-08-09 07:47:35,359 - distributed.worker.memory - WARNING - Worker is at 87% memory usage. Pausing worker.  Process memory: 1.63 GiB -- Worker memory limit: 1.86 GiB
2024-08-09 07:47:35,366 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 1.63 GiB -- Worker memory limit: 1.86 GiB
2024-08-09 07:47:35,459 - distributed.worker.memory - WARNING - Worker is at 37% memory usage. Resuming worker. Process memory: 713.74 MiB -- Worker memory limit: 1.86 GiB
slurmstepd: error: *** JOB 2331192 ON cl-node004 CANCELLED AT 2024-08-09T07:47:50 ***
2024-08-09 07:47:50,912 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:36319. Reason: scheduler-close
2024-08-09 07:47:50,912 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:37727. Reason: scheduler-close
2024-08-09 07:47:50,912 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:39265. Reason: scheduler-close
2024-08-09 07:47:50,913 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:37606. Reason: scheduler-close
2024-08-09 07:47:50,913 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:35917. Reason: scheduler-close
2024-08-09 07:47:50,912 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:46862. Reason: scheduler-close
2024-08-09 07:47:50,913 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:38031. Reason: scheduler-close
2024-08-09 07:47:50,913 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.4:43579. Reason: scheduler-close
2024-08-09 07:47:50,915 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:37705'. Reason: scheduler-close
2024-08-09 07:47:50,914 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:60008 remote=tcp://10.34.59.1:43219>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:60008 remote=tcp://10.34.59.1:43219>: Stream is closed
2024-08-09 07:47:50,915 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:60016 remote=tcp://10.34.59.1:43219>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:60016 remote=tcp://10.34.59.1:43219>: Stream is closed
2024-08-09 07:47:50,915 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:60009 remote=tcp://10.34.59.1:43219>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.4:60009 remote=tcp://10.34.59.1:43219>: Stream is closed
2024-08-09 07:47:50,917 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:37095'. Reason: scheduler-close
2024-08-09 07:47:50,918 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:33003'. Reason: scheduler-close
2024-08-09 07:47:50,918 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,918 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:34374'. Reason: scheduler-close
2024-08-09 07:47:50,918 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,918 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:44285'. Reason: scheduler-close
2024-08-09 07:47:50,920 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,920 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,920 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:40506'. Reason: scheduler-close
2024-08-09 07:47:50,920 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,920 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,920 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:40007'. Reason: scheduler-close
2024-08-09 07:47:50,921 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,921 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.4:33401'. Reason: scheduler-close
2024-08-09 07:47:50,921 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,921 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,921 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,924 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,924 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,924 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,924 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:50,924 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43219; closing.
2024-08-09 07:47:50,925 - distributed.nanny - INFO - Worker closed
