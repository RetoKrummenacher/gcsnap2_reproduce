2024-08-09 07:26:27,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35159'
2024-08-09 07:26:27,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41021'
2024-08-09 07:26:27,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43813'
2024-08-09 07:26:27,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35865'
2024-08-09 07:26:27,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36190'
2024-08-09 07:26:27,648 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37660'
2024-08-09 07:26:27,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39945'
2024-08-09 07:26:27,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39690'
2024-08-09 07:26:27,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42173'
2024-08-09 07:26:27,658 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40967'
2024-08-09 07:26:27,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45545'
2024-08-09 07:26:27,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39406'
2024-08-09 07:26:27,659 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40066'
2024-08-09 07:26:27,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:32933'
2024-08-09 07:26:27,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:32814'
2024-08-09 07:26:27,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35776'
2024-08-09 07:26:28,751 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37302
2024-08-09 07:26:28,751 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39520
2024-08-09 07:26:28,752 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37302
2024-08-09 07:26:28,752 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39520
2024-08-09 07:26:28,752 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 07:26:28,752 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 07:26:28,752 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41730
2024-08-09 07:26:28,752 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43930
2024-08-09 07:26:28,752 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,752 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,752 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,752 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,752 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,752 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,752 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-du7rjr5s
2024-08-09 07:26:28,752 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9xgjby9y
2024-08-09 07:26:28,752 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,752 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,753 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41593
2024-08-09 07:26:28,753 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41593
2024-08-09 07:26:28,753 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 07:26:28,753 - distributed.worker - INFO -          dashboard at:           10.34.59.2:32813
2024-08-09 07:26:28,753 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,753 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,753 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,753 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y41nmdmu
2024-08-09 07:26:28,754 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,757 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39892
2024-08-09 07:26:28,757 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39892
2024-08-09 07:26:28,757 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 07:26:28,757 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35765
2024-08-09 07:26:28,757 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,757 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,757 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,757 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yty39auf
2024-08-09 07:26:28,757 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,760 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37641
2024-08-09 07:26:28,760 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37641
2024-08-09 07:26:28,760 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 07:26:28,760 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33014
2024-08-09 07:26:28,760 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,760 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,760 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44038
2024-08-09 07:26:28,760 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,760 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44038
2024-08-09 07:26:28,760 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,760 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q3xvdvha
2024-08-09 07:26:28,760 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 07:26:28,760 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33443
2024-08-09 07:26:28,760 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,761 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,761 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,761 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,761 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-resjbc67
2024-08-09 07:26:28,761 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,762 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44613
2024-08-09 07:26:28,762 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44613
2024-08-09 07:26:28,762 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 07:26:28,762 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42126
2024-08-09 07:26:28,763 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,763 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,763 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-upoh161f
2024-08-09 07:26:28,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,767 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33098
2024-08-09 07:26:28,767 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33098
2024-08-09 07:26:28,767 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 07:26:28,767 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42698
2024-08-09 07:26:28,767 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,767 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,767 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,767 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,767 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35775
2024-08-09 07:26:28,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cq551y9d
2024-08-09 07:26:28,767 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35775
2024-08-09 07:26:28,767 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,767 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 07:26:28,767 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43499
2024-08-09 07:26:28,767 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,767 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,767 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,767 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vsauribw
2024-08-09 07:26:28,768 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,770 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36141
2024-08-09 07:26:28,770 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36141
2024-08-09 07:26:28,770 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 07:26:28,770 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45972
2024-08-09 07:26:28,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,771 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,771 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,771 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,771 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qtzaqxwo
2024-08-09 07:26:28,771 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,773 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35640
2024-08-09 07:26:28,773 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35640
2024-08-09 07:26:28,773 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 07:26:28,773 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46664
2024-08-09 07:26:28,773 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,773 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,773 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,773 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-es9nwln1
2024-08-09 07:26:28,773 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,774 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46613
2024-08-09 07:26:28,775 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46613
2024-08-09 07:26:28,775 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 07:26:28,775 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34521
2024-08-09 07:26:28,775 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38318
2024-08-09 07:26:28,775 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,775 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38318
2024-08-09 07:26:28,775 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,775 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 07:26:28,775 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,775 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44663
2024-08-09 07:26:28,775 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,775 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o5g0whoj
2024-08-09 07:26:28,775 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,775 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,775 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,775 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,775 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3xo721vk
2024-08-09 07:26:28,776 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,776 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35451
2024-08-09 07:26:28,776 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35451
2024-08-09 07:26:28,776 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 07:26:28,776 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38410
2024-08-09 07:26:28,776 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,776 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,776 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,776 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pz_0qgb9
2024-08-09 07:26:28,776 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,781 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37194
2024-08-09 07:26:28,781 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37194
2024-08-09 07:26:28,781 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 07:26:28,781 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44796
2024-08-09 07:26:28,781 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44348
2024-08-09 07:26:28,781 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44796
2024-08-09 07:26:28,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,781 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,781 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 07:26:28,781 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,781 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38964
2024-08-09 07:26:28,781 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46795
2024-08-09 07:26:28,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-amuov4ab
2024-08-09 07:26:28,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,782 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:26:28,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:28,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:26:28,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nmn2rc5j
2024-08-09 07:26:28,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,134 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,135 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,135 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,135 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,136 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,137 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,137 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,138 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,138 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,138 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,138 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,138 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,139 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,139 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,139 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,139 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,140 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,140 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,140 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,140 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,141 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,142 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,142 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,149 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,149 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,150 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,150 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,151 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,151 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,151 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,151 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,151 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,151 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,152 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,152 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,153 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,154 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,154 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,155 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,156 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,156 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,157 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,157 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,157 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,158 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,158 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,158 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,159 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
2024-08-09 07:26:29,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:26:29,159 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46795
2024-08-09 07:26:29,159 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:26:29,160 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46795
slurmstepd: error: *** JOB 2331158 ON cl-node002 CANCELLED AT 2024-08-09T07:26:48 ***
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39520. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44613. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44038. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35640. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39892. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37302. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37641. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36141. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41593. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37194. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46613. Reason: scheduler-close
2024-08-09 07:26:48,970 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44796. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35775. Reason: scheduler-close
2024-08-09 07:26:48,969 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38318. Reason: scheduler-close
2024-08-09 07:26:48,970 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35451. Reason: scheduler-close
2024-08-09 07:26:48,970 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33098. Reason: scheduler-close
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42522 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42522 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42530 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42530 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42540 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42540 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42532 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42532 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42548 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42548 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42550 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42550 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42542 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42542 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42538 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42538 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42520 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42520 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42528 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42528 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42524 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42524 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42546 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42546 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42534 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42534 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,971 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42526 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42526 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42544 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42544 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,973 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42536 remote=tcp://10.34.59.1:46795>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:42536 remote=tcp://10.34.59.1:46795>: Stream is closed
2024-08-09 07:26:48,976 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35865'. Reason: scheduler-close
2024-08-09 07:26:48,979 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,979 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,981 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36190'. Reason: scheduler-close
2024-08-09 07:26:48,981 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39406'. Reason: scheduler-close
2024-08-09 07:26:48,982 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39690'. Reason: scheduler-close
2024-08-09 07:26:48,982 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:32814'. Reason: scheduler-close
2024-08-09 07:26:48,983 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43813'. Reason: scheduler-close
2024-08-09 07:26:48,983 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40967'. Reason: scheduler-close
2024-08-09 07:26:48,983 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,983 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35776'. Reason: scheduler-close
2024-08-09 07:26:48,983 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,984 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:32933'. Reason: scheduler-close
2024-08-09 07:26:48,984 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,984 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,984 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,984 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35159'. Reason: scheduler-close
2024-08-09 07:26:48,984 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,984 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,984 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37660'. Reason: scheduler-close
2024-08-09 07:26:48,984 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,985 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,985 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,985 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,985 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,985 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,986 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,986 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,986 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,987 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,987 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,987 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,988 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,988 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41021'. Reason: scheduler-close
2024-08-09 07:26:48,988 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40066'. Reason: scheduler-close
2024-08-09 07:26:48,988 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42173'. Reason: scheduler-close
2024-08-09 07:26:48,989 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45545'. Reason: scheduler-close
2024-08-09 07:26:48,989 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39945'. Reason: scheduler-close
2024-08-09 07:26:48,990 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,990 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,990 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,990 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,991 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,991 - distributed.nanny - INFO - Worker closed
2024-08-09 07:26:48,992 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46795; closing.
2024-08-09 07:26:48,992 - distributed.nanny - INFO - Worker closed
