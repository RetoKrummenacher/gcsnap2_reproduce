2024-08-09 04:01:37,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36304'
2024-08-09 04:01:37,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33144'
2024-08-09 04:01:37,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40456'
2024-08-09 04:01:37,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43256'
2024-08-09 04:01:37,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34660'
2024-08-09 04:01:37,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43052'
2024-08-09 04:01:37,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35541'
2024-08-09 04:01:37,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41209'
2024-08-09 04:01:37,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42772'
2024-08-09 04:01:37,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42340'
2024-08-09 04:01:37,022 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44221'
2024-08-09 04:01:37,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:32870'
2024-08-09 04:01:37,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46712'
2024-08-09 04:01:37,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45450'
2024-08-09 04:01:37,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42395'
2024-08-09 04:01:37,034 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45142'
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33352
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42172
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45168
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42172
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33352
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35706
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44108
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45168
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39777
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35706
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44002
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44329
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34387
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35086
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44108
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39777
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42374
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46257
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34387
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40657
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35086
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34909
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42374
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33146
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36485
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40657
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44725
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43299
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38670
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40813
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39167
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44224
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44725
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33903
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35921
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36485
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2z3ufdxl
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vta09khe
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38670
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35572
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44224
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c40fw511
2024-08-09 04:01:38,269 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35921
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43597
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33029
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tnqfuqk1
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41043
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-shne8530
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hh1agar7
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34920
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36741
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v6p91wtj
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-elkrjla5
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-upuzxax4
2024-08-09 04:01:38,269 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xfq3d_42
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-krrlbg43
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kvqp2sa0
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-svypwsla
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4hufhnun
2024-08-09 04:01:38,269 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3lysh48b
2024-08-09 04:01:38,270 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,270 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,270 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,270 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43648
2024-08-09 04:01:38,270 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43648
2024-08-09 04:01:38,270 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 04:01:38,270 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35966
2024-08-09 04:01:38,270 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,271 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,271 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:01:38,271 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:01:38,271 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ikk9msl
2024-08-09 04:01:38,271 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,732 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,732 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,733 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,733 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,733 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,733 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,733 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,733 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,734 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,734 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,734 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,734 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,734 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,735 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,735 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,736 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,736 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,736 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,736 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,737 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,737 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,737 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,737 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,738 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,738 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,738 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,739 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,739 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,739 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,739 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,740 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,740 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,741 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,741 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,741 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,742 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,743 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,743 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,750 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,751 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,751 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,752 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,752 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,752 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,752 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,752 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,753 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,753 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,754 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,754 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,754 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
2024-08-09 04:01:38,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:01:38,755 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:44283
2024-08-09 04:01:38,755 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:01:38,756 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:44283
slurmstepd: error: *** JOB 2328829 ON cl-node002 CANCELLED AT 2024-08-09T04:01:46 ***
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44108. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35921. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35706. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44725. Reason: scheduler-close
2024-08-09 04:01:46,961 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40657. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36485. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33352. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39777. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38670. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44224. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43648. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42374. Reason: scheduler-close
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34387. Reason: scheduler-close
2024-08-09 04:01:46,964 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55618 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55618 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,964 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55636 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55636 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45168. Reason: scheduler-close
2024-08-09 04:01:46,964 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55642 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55642 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35086. Reason: scheduler-close
2024-08-09 04:01:46,964 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55630 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55630 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,965 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55622 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55622 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,962 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42172. Reason: scheduler-close
2024-08-09 04:01:46,965 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55638 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55638 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,966 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55624 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55624 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,964 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55634 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55634 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,966 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55616 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55616 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,964 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55613 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55613 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,966 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55626 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55626 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,967 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55628 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55628 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,968 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55632 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55632 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,969 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42772'. Reason: scheduler-close
2024-08-09 04:01:46,967 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55612 remote=tcp://10.34.59.1:44283>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:55612 remote=tcp://10.34.59.1:44283>: Stream is closed
2024-08-09 04:01:46,970 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43052'. Reason: scheduler-close
2024-08-09 04:01:46,970 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43256'. Reason: scheduler-close
2024-08-09 04:01:46,971 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40456'. Reason: scheduler-close
2024-08-09 04:01:46,971 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42395'. Reason: scheduler-close
2024-08-09 04:01:46,971 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33144'. Reason: scheduler-close
2024-08-09 04:01:46,972 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,972 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,972 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,972 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,972 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,972 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,973 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,973 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:32870'. Reason: scheduler-close
2024-08-09 04:01:46,973 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,973 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,973 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,974 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45142'. Reason: scheduler-close
2024-08-09 04:01:46,974 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,974 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,975 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41209'. Reason: scheduler-close
2024-08-09 04:01:46,975 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,975 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,976 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,976 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,977 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42340'. Reason: scheduler-close
2024-08-09 04:01:46,977 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,977 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,977 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35541'. Reason: scheduler-close
2024-08-09 04:01:46,977 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46712'. Reason: scheduler-close
2024-08-09 04:01:46,978 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45450'. Reason: scheduler-close
2024-08-09 04:01:46,978 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36304'. Reason: scheduler-close
2024-08-09 04:01:46,979 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44221'. Reason: scheduler-close
2024-08-09 04:01:46,979 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,980 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,980 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,980 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34660'. Reason: scheduler-close
2024-08-09 04:01:46,980 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,980 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,980 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,980 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,980 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,981 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,981 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,981 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,981 - distributed.nanny - INFO - Worker closed
2024-08-09 04:01:46,982 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:44283; closing.
2024-08-09 04:01:46,982 - distributed.nanny - INFO - Worker closed
