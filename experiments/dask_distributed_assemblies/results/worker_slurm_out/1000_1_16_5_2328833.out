2024-08-09 04:03:36,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39179'
2024-08-09 04:03:36,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34053'
2024-08-09 04:03:36,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38552'
2024-08-09 04:03:36,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45392'
2024-08-09 04:03:36,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44038'
2024-08-09 04:03:36,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37564'
2024-08-09 04:03:36,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38932'
2024-08-09 04:03:36,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41326'
2024-08-09 04:03:36,931 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46342'
2024-08-09 04:03:36,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34848'
2024-08-09 04:03:36,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44690'
2024-08-09 04:03:36,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43877'
2024-08-09 04:03:36,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37199'
2024-08-09 04:03:36,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46619'
2024-08-09 04:03:36,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33675'
2024-08-09 04:03:36,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34496'
2024-08-09 04:03:38,094 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43625
2024-08-09 04:03:38,094 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43621
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43625
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:32950
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43621
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40589
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:32950
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42547
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33698
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42547
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:32775
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38508
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33424
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:32775
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33424
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33468
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jy8lj6zu
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3b6z_8gq
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35722
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37824
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oq38e734
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34994
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b8lqee92
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34994
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zfrrizgk
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7jr60rwu
2024-08-09 04:03:38,095 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33147
2024-08-09 04:03:38,095 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,095 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,095 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4c9h85gp
2024-08-09 04:03:38,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,097 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39150
2024-08-09 04:03:38,097 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46443
2024-08-09 04:03:38,097 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39150
2024-08-09 04:03:38,097 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46443
2024-08-09 04:03:38,097 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 04:03:38,097 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 04:03:38,097 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36679
2024-08-09 04:03:38,098 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,098 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40295
2024-08-09 04:03:38,097 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44960
2024-08-09 04:03:38,098 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,098 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38166
2024-08-09 04:03:38,098 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38317
2024-08-09 04:03:38,098 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35497
2024-08-09 04:03:38,098 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,099 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44960
2024-08-09 04:03:38,099 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38166
2024-08-09 04:03:38,099 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 04:03:38,099 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,099 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38317
2024-08-09 04:03:38,099 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35497
2024-08-09 04:03:38,099 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,099 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 04:03:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5s_cgo_j
2024-08-09 04:03:38,099 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40483
2024-08-09 04:03:38,099 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 04:03:38,099 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,099 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 04:03:38,099 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,099 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36331
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-loiy6_y8
2024-08-09 04:03:38,099 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40072
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36053
2024-08-09 04:03:38,099 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,099 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,099 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,099 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,099 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7q5_9a7y
2024-08-09 04:03:38,099 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,099 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,099 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,099 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z6bc1evs
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2n1089ah
2024-08-09 04:03:38,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3lbir_lg
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,105 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42786
2024-08-09 04:03:38,105 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42786
2024-08-09 04:03:38,105 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45651
2024-08-09 04:03:38,105 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 04:03:38,105 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45651
2024-08-09 04:03:38,105 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44287
2024-08-09 04:03:38,105 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 04:03:38,105 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,105 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,105 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44928
2024-08-09 04:03:38,105 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,105 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,105 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,105 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sjgwwwl0
2024-08-09 04:03:38,105 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,105 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,105 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wxgi8yk0
2024-08-09 04:03:38,105 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,106 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43466
2024-08-09 04:03:38,106 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43466
2024-08-09 04:03:38,106 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 04:03:38,106 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33560
2024-08-09 04:03:38,106 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,106 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,106 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:38,106 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:38,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kdyjxd1n
2024-08-09 04:03:38,106 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,478 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,478 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,478 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,479 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,479 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,479 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,480 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,480 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,480 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,480 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,480 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,481 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,481 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,481 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,481 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,482 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,483 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,483 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,483 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,485 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,486 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,486 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,486 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,486 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,487 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,487 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,487 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,487 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,488 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,488 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,488 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,488 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,488 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,489 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,489 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,489 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,490 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,490 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,490 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,490 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,490 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,490 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,491 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,495 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,495 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,496 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,500 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,501 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,501 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
2024-08-09 04:03:38,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:38,503 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41970
2024-08-09 04:03:38,504 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:38,504 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41970
slurmstepd: error: *** JOB 2328833 ON cl-node002 CANCELLED AT 2024-08-09T04:03:46 ***
2024-08-09 04:03:46,690 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43621. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:32950. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:32775. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43625. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42547. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35497. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43466. Reason: scheduler-close
2024-08-09 04:03:46,690 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34994. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45651. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42786. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46443. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44960. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39150. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38166. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33424. Reason: scheduler-close
2024-08-09 04:03:46,691 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38317. Reason: scheduler-close
2024-08-09 04:03:46,692 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40648 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40648 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40670 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40670 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,694 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44038'. Reason: scheduler-close
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40660 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40660 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40654 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40654 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,694 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41326'. Reason: scheduler-close
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40666 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40666 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40674 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40674 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40672 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40672 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40658 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40658 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,693 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40668 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40668 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,694 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40662 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40662 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,694 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40676 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40676 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,696 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39179'. Reason: scheduler-close
2024-08-09 04:03:46,696 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,694 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40664 remote=tcp://10.34.59.1:41970>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:40664 remote=tcp://10.34.59.1:41970>: Stream is closed
2024-08-09 04:03:46,696 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34496'. Reason: scheduler-close
2024-08-09 04:03:46,696 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,696 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,696 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,698 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,698 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,699 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,699 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,699 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38932'. Reason: scheduler-close
2024-08-09 04:03:46,699 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46342'. Reason: scheduler-close
2024-08-09 04:03:46,700 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34848'. Reason: scheduler-close
2024-08-09 04:03:46,700 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46619'. Reason: scheduler-close
2024-08-09 04:03:46,700 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44690'. Reason: scheduler-close
2024-08-09 04:03:46,701 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,701 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,701 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,701 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,701 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,701 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,702 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,702 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,702 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,702 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,703 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37564'. Reason: scheduler-close
2024-08-09 04:03:46,704 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37199'. Reason: scheduler-close
2024-08-09 04:03:46,704 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34053'. Reason: scheduler-close
2024-08-09 04:03:46,705 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38552'. Reason: scheduler-close
2024-08-09 04:03:46,705 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43877'. Reason: scheduler-close
2024-08-09 04:03:46,705 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45392'. Reason: scheduler-close
2024-08-09 04:03:46,705 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,706 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,706 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33675'. Reason: scheduler-close
2024-08-09 04:03:46,706 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,706 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,706 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,707 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,707 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,707 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,708 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,708 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,708 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,709 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:46,709 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:41970; closing.
2024-08-09 04:03:46,709 - distributed.nanny - INFO - Worker closed
