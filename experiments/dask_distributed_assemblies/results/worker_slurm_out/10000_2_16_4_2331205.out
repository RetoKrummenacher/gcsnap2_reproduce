2024-08-09 07:50:58,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40962'
2024-08-09 07:50:58,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43849'
2024-08-09 07:50:58,128 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45625'
2024-08-09 07:50:58,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34284'
2024-08-09 07:50:58,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37308'
2024-08-09 07:50:58,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44585'
2024-08-09 07:50:58,136 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33243'
2024-08-09 07:50:58,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41021'
2024-08-09 07:50:58,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40669'
2024-08-09 07:50:58,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44677'
2024-08-09 07:50:58,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41909'
2024-08-09 07:50:58,138 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34431'
2024-08-09 07:50:58,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39450'
2024-08-09 07:50:58,139 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41573'
2024-08-09 07:50:58,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38290'
2024-08-09 07:50:58,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34048'
2024-08-09 07:50:58,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2by40gdw', purging
2024-08-09 07:50:58,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0c75bp4f', purging
2024-08-09 07:50:58,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jl82ryhm', purging
2024-08-09 07:50:58,740 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zjqg8isr', purging
2024-08-09 07:50:58,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0d10ae6y', purging
2024-08-09 07:50:58,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uv2f8lby', purging
2024-08-09 07:50:58,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-571byi4l', purging
2024-08-09 07:50:58,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tj055g95', purging
2024-08-09 07:50:58,741 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nu039t9n', purging
2024-08-09 07:50:58,742 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-mcwe7jgl', purging
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40938
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43305
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40938
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37275
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34382
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38367
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43305
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37275
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34382
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38367
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40788
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43419
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42490
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40788
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37431
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44912
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43419
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45346
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35273
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39415
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40180
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37431
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35273
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37138
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43807
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40236
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42666
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dhwu1q9f
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46009
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36456
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42803
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40236
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44316
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-14he99tt
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-daohtmjp
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42666
2024-08-09 07:50:59,285 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-plioxf0s
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42803
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5b6rimmi
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44316
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33185
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,285 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 07:50:59,285 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,285 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40466
2024-08-09 07:50:59,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gmtypu_5
2024-08-09 07:50:59,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,286 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44986
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gjq5cidf
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,286 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42478
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a0y8o5n6
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lxeixig9
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pewerhd2
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-51zzbrd_
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34490
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0yd092tq
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iltip_a9
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34490
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 07:50:59,286 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41160
2024-08-09 07:50:59,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,286 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r468sexd
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43861
2024-08-09 07:50:59,286 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43861
2024-08-09 07:50:59,286 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 07:50:59,286 - distributed.worker - INFO -          dashboard at:           10.34.59.2:32986
2024-08-09 07:50:59,286 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,286 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,286 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37495
2024-08-09 07:50:59,286 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,286 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37495
2024-08-09 07:50:59,287 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,287 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 07:50:59,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zbk2ieud
2024-08-09 07:50:59,287 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41085
2024-08-09 07:50:59,287 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,287 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,287 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,287 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:50:59,287 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:50:59,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jw2hcn4o
2024-08-09 07:50:59,287 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,737 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,738 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,738 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,738 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,739 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,739 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,739 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,739 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,739 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,739 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,740 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,740 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,740 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,741 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,741 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,741 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,741 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,741 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,742 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,742 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,742 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,743 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,743 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,743 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,743 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,743 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,744 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,744 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,745 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,745 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,745 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,745 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,745 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,745 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,746 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,746 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,746 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,746 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,747 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,747 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,747 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,747 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,748 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,748 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,748 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,751 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,752 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,752 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,753 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,753 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:50:59,753 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
2024-08-09 07:50:59,754 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:34150
2024-08-09 07:50:59,754 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:50:59,754 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:34150
slurmstepd: error: *** JOB 2331205 ON cl-node002 CANCELLED AT 2024-08-09T07:51:11 ***
2024-08-09 07:51:11,731 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34382. Reason: scheduler-close
2024-08-09 07:51:11,731 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42803. Reason: scheduler-close
2024-08-09 07:51:11,731 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43305. Reason: scheduler-close
2024-08-09 07:51:11,731 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43419. Reason: scheduler-close
2024-08-09 07:51:11,731 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44316. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42666. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37431. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37495. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37275. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40788. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43861. Reason: scheduler-close
2024-08-09 07:51:11,732 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35273. Reason: scheduler-close
2024-08-09 07:51:11,733 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34490. Reason: scheduler-close
2024-08-09 07:51:11,733 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40938. Reason: scheduler-close
2024-08-09 07:51:11,734 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40236. Reason: scheduler-close
2024-08-09 07:51:11,736 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38367. Reason: scheduler-close
2024-08-09 07:51:11,735 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34330 remote=tcp://10.34.59.1:34150>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34330 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,735 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34322 remote=tcp://10.34.59.1:34150>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34322 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,735 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34304 remote=tcp://10.34.59.1:34150>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34304 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,737 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34312 remote=tcp://10.34.59.1:34150>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34312 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,740 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45625'. Reason: scheduler-close
2024-08-09 07:51:11,741 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39450'. Reason: scheduler-close
2024-08-09 07:51:11,741 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37308'. Reason: scheduler-close
2024-08-09 07:51:11,742 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41021'. Reason: scheduler-close
2024-08-09 07:51:11,742 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34431'. Reason: scheduler-close
2024-08-09 07:51:11,742 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.2:34366 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,742 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44677'. Reason: scheduler-close
2024-08-09 07:51:11,743 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43849'. Reason: scheduler-close
2024-08-09 07:51:11,743 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34284'. Reason: scheduler-close
2024-08-09 07:51:11,743 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38290'. Reason: scheduler-close
2024-08-09 07:51:11,744 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34048'. Reason: scheduler-close
2024-08-09 07:51:11,744 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,744 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,744 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41573'. Reason: scheduler-close
2024-08-09 07:51:11,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.2:34338 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,744 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,745 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,743 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.2:34340 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,745 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,745 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,745 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,744 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.2:34354 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,745 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,744 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 452, in retry_operation
    return await retry(
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/utils_comm.py", line 431, in retry
    return await coro()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://10.34.59.2:34336 remote=tcp://10.34.59.1:34150>: Stream is closed
2024-08-09 07:51:11,746 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40669'. Reason: scheduler-close
2024-08-09 07:51:11,746 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,746 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,746 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,746 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,746 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,746 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,747 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,747 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33243'. Reason: scheduler-close
2024-08-09 07:51:11,747 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,747 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,747 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,747 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,747 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,747 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,748 - distributed.nanny - INFO - Worker closed
2024-08-09 07:51:11,748 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40962'. Reason: scheduler-close
2024-08-09 07:51:11,749 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44585'. Reason: scheduler-close
2024-08-09 07:51:11,749 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:34150; closing.
2024-08-09 07:51:11,749 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41909'. Reason: scheduler-close
2024-08-09 07:51:11,749 - distributed.nanny - INFO - Worker closed
