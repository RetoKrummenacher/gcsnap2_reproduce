2024-08-09 04:02:36,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44698'
2024-08-09 04:02:36,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38265'
2024-08-09 04:02:36,986 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44997'
2024-08-09 04:02:36,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41744'
2024-08-09 04:02:36,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35746'
2024-08-09 04:02:37,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42270'
2024-08-09 04:02:37,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36215'
2024-08-09 04:02:37,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34335'
2024-08-09 04:02:37,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45159'
2024-08-09 04:02:37,004 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38203'
2024-08-09 04:02:37,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41123'
2024-08-09 04:02:37,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34232'
2024-08-09 04:02:37,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36359'
2024-08-09 04:02:37,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41367'
2024-08-09 04:02:37,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38362'
2024-08-09 04:02:37,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33417'
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37026
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35258
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36740
2024-08-09 04:02:38,132 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37026
2024-08-09 04:02:38,132 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35258
2024-08-09 04:02:38,132 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36740
2024-08-09 04:02:38,132 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 04:02:38,132 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37164
2024-08-09 04:02:38,132 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 04:02:38,132 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34760
2024-08-09 04:02:38,132 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34792
2024-08-09 04:02:38,132 - distributed.worker - INFO -          dashboard at:           10.34.59.2:32937
2024-08-09 04:02:38,132 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,132 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37164
2024-08-09 04:02:38,132 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,132 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,132 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,132 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 04:02:38,132 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,132 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,132 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,132 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,132 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34897
2024-08-09 04:02:38,132 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,132 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,132 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,132 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-77pyz35p
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40033
2024-08-09 04:02:38,132 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-biafxur4
2024-08-09 04:02:38,132 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2cs1r5yr
2024-08-09 04:02:38,132 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40033
2024-08-09 04:02:38,132 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,132 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q_ehhhnq
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34508
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39679
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46610
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44502
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39679
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35592
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46610
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44502
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35592
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-287t4q1z
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42950
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43062
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35195
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45885
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41036
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41036
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y1bn6qa2
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33723
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46585
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_roivtd8
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ve2rz96l
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j9nt_q_k
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46585
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42369
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43433
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42369
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x9hkdw8x
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33510
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40753
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-daga9a0c
2024-08-09 04:02:38,133 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,133 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40753
2024-08-09 04:02:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,133 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,133 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 04:02:38,134 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ihmr6zvm
2024-08-09 04:02:38,134 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39978
2024-08-09 04:02:38,134 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,134 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,134 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,134 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,134 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,134 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39168
2024-08-09 04:02:38,134 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-__fvr5rg
2024-08-09 04:02:38,134 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39168
2024-08-09 04:02:38,134 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,134 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 04:02:38,134 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40385
2024-08-09 04:02:38,135 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,135 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,136 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,136 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lhtyidbl
2024-08-09 04:02:38,137 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,138 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36032
2024-08-09 04:02:38,138 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36032
2024-08-09 04:02:38,138 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 04:02:38,138 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41737
2024-08-09 04:02:38,138 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,138 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,138 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,138 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,139 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y8ibcw5n
2024-08-09 04:02:38,139 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,143 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39322
2024-08-09 04:02:38,143 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39322
2024-08-09 04:02:38,143 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 04:02:38,143 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44730
2024-08-09 04:02:38,143 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,143 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,143 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:38,143 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:38,143 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gqf3miwn
2024-08-09 04:02:38,143 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,524 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,525 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,525 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,525 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,526 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,526 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,526 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,527 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,527 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,527 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,527 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,527 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,528 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,528 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,528 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,529 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,529 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,529 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,529 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,529 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,529 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,529 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,530 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,530 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,530 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,530 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,531 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,531 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,531 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,532 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,532 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,532 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,532 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,532 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,533 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,533 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,544 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,544 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,544 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,545 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,545 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,545 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,545 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,546 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,546 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,546 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,547 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,547 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,547 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,547 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,547 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,547 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:38,548 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
2024-08-09 04:02:38,548 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:36956
2024-08-09 04:02:38,548 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:38,549 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:36956
slurmstepd: error: *** JOB 2328831 ON cl-node002 CANCELLED AT 2024-08-09T04:02:46 ***
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35258. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46610. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39168. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36740. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37164. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39679. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41036. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37026. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40033. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36032. Reason: scheduler-close
2024-08-09 04:02:46,789 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46585. Reason: scheduler-close
2024-08-09 04:02:46,789 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39322. Reason: scheduler-close
2024-08-09 04:02:46,789 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35592. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40753. Reason: scheduler-close
2024-08-09 04:02:46,790 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44502. Reason: scheduler-close
2024-08-09 04:02:46,793 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42369. Reason: scheduler-close
2024-08-09 04:02:46,792 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34992 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34992 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,792 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35020 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35020 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,793 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35016 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35016 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,793 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35022 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35022 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,793 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35012 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35012 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,793 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35004 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35004 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,793 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35008 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35008 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,794 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35014 remote=tcp://10.34.59.1:36956>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:35014 remote=tcp://10.34.59.1:36956>: Stream is closed
2024-08-09 04:02:46,795 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41367'. Reason: scheduler-close
2024-08-09 04:02:46,798 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,798 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,799 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45159'. Reason: scheduler-close
2024-08-09 04:02:46,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34335'. Reason: scheduler-close
2024-08-09 04:02:46,800 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44698'. Reason: scheduler-close
2024-08-09 04:02:46,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41744'. Reason: scheduler-close
2024-08-09 04:02:46,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44997'. Reason: scheduler-close
2024-08-09 04:02:46,801 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38265'. Reason: scheduler-close
2024-08-09 04:02:46,802 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34232'. Reason: scheduler-close
2024-08-09 04:02:46,802 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,802 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,802 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,802 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,802 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,802 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,803 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,803 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,803 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,803 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,803 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,803 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,804 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,804 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,805 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35746'. Reason: scheduler-close
2024-08-09 04:02:46,806 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33417'. Reason: scheduler-close
2024-08-09 04:02:46,806 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41123'. Reason: scheduler-close
2024-08-09 04:02:46,806 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36359'. Reason: scheduler-close
2024-08-09 04:02:46,807 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38203'. Reason: scheduler-close
2024-08-09 04:02:46,807 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36215'. Reason: scheduler-close
2024-08-09 04:02:46,807 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42270'. Reason: scheduler-close
2024-08-09 04:02:46,808 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38362'. Reason: scheduler-close
2024-08-09 04:02:46,808 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,808 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,808 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,808 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,809 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,809 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,809 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,809 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,809 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,809 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,809 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,810 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,810 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,810 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:36956; closing.
2024-08-09 04:02:46,810 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:46,810 - distributed.nanny - INFO - Worker closed
