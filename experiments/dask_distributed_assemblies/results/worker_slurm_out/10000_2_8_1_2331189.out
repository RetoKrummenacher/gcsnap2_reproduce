2024-08-09 07:46:56,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35398'
2024-08-09 07:46:56,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44935'
2024-08-09 07:46:56,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45024'
2024-08-09 07:46:56,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33570'
2024-08-09 07:46:56,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42892'
2024-08-09 07:46:56,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40540'
2024-08-09 07:46:56,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:32836'
2024-08-09 07:46:56,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35096'
2024-08-09 07:46:57,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-l8q1jzii', purging
2024-08-09 07:46:57,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8kuv2fjm', purging
2024-08-09 07:46:57,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o0ns1jcz', purging
2024-08-09 07:46:57,264 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-47_jo81h', purging
2024-08-09 07:46:57,750 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37967
2024-08-09 07:46:57,750 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37967
2024-08-09 07:46:57,750 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 07:46:57,750 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37734
2024-08-09 07:46:57,750 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,750 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,750 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,750 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,750 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mp30m6nc
2024-08-09 07:46:57,750 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,753 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34723
2024-08-09 07:46:57,753 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34723
2024-08-09 07:46:57,753 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 07:46:57,753 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45428
2024-08-09 07:46:57,753 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,753 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,753 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,753 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bj7cyipo
2024-08-09 07:46:57,753 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,754 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37824
2024-08-09 07:46:57,754 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37824
2024-08-09 07:46:57,754 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 07:46:57,754 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41273
2024-08-09 07:46:57,754 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,754 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,754 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,754 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,755 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-98a_7hl6
2024-08-09 07:46:57,755 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,755 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34447
2024-08-09 07:46:57,755 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34447
2024-08-09 07:46:57,755 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 07:46:57,755 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37174
2024-08-09 07:46:57,755 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,756 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,756 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,756 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,756 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rxiz3wkx
2024-08-09 07:46:57,756 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33159
2024-08-09 07:46:57,756 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,756 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33159
2024-08-09 07:46:57,756 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 07:46:57,756 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39868
2024-08-09 07:46:57,756 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,756 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,757 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,757 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2xw5kxqq
2024-08-09 07:46:57,757 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,759 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44663
2024-08-09 07:46:57,759 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44663
2024-08-09 07:46:57,759 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 07:46:57,759 - distributed.worker - INFO -          dashboard at:           10.34.59.2:32818
2024-08-09 07:46:57,759 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,759 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,759 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,759 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,759 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mgeltst7
2024-08-09 07:46:57,759 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,760 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34937
2024-08-09 07:46:57,760 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34937
2024-08-09 07:46:57,760 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 07:46:57,760 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41379
2024-08-09 07:46:57,760 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,760 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,760 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,760 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,760 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fybyexk7
2024-08-09 07:46:57,760 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,761 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35702
2024-08-09 07:46:57,761 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35702
2024-08-09 07:46:57,761 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 07:46:57,762 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33616
2024-08-09 07:46:57,762 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:43871
2024-08-09 07:46:57,762 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:57,762 - distributed.worker - INFO -               Threads:                          1
2024-08-09 07:46:57,762 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 07:46:57,762 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7zrlnk5m
2024-08-09 07:46:57,762 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,121 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,122 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,122 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,122 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,122 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,123 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,123 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,123 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,143 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,144 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,144 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,144 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,145 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,145 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,145 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,145 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,145 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,145 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,146 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,146 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,146 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,147 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,147 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,147 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,147 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 07:46:58,148 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
2024-08-09 07:46:58,148 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:43871
2024-08-09 07:46:58,148 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 07:46:58,149 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:43871
slurmstepd: error: *** JOB 2331189 ON cl-node002 CANCELLED AT 2024-08-09T07:47:17 ***
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34447. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35702. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37824. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33159. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34723. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37967. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34937. Reason: scheduler-close
2024-08-09 07:47:17,902 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44663. Reason: scheduler-close
2024-08-09 07:47:17,903 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34876 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34876 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34878 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34878 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34874 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34874 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34872 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34872 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34884 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34884 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34880 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34880 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34882 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34882 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,904 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34870 remote=tcp://10.34.59.1:43871>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:34870 remote=tcp://10.34.59.1:43871>: Stream is closed
2024-08-09 07:47:17,908 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42892'. Reason: scheduler-close
2024-08-09 07:47:17,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,911 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,911 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40540'. Reason: scheduler-close
2024-08-09 07:47:17,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45024'. Reason: scheduler-close
2024-08-09 07:47:17,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35398'. Reason: scheduler-close
2024-08-09 07:47:17,912 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33570'. Reason: scheduler-close
2024-08-09 07:47:17,913 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:32836'. Reason: scheduler-close
2024-08-09 07:47:17,913 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35096'. Reason: scheduler-close
2024-08-09 07:47:17,914 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,914 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,914 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,914 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,914 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44935'. Reason: scheduler-close
2024-08-09 07:47:17,914 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,915 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,915 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,915 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,916 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,916 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,916 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,916 - distributed.nanny - INFO - Worker closed
2024-08-09 07:47:17,917 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:43871; closing.
2024-08-09 07:47:17,918 - distributed.nanny - INFO - Worker closed
