2024-08-09 05:57:23,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44384'
2024-08-09 05:57:23,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34869'
2024-08-09 05:57:23,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41539'
2024-08-09 05:57:23,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39140'
2024-08-09 05:57:23,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42068'
2024-08-09 05:57:23,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35348'
2024-08-09 05:57:23,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36034'
2024-08-09 05:57:23,664 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37752'
2024-08-09 05:57:23,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43223'
2024-08-09 05:57:23,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35969'
2024-08-09 05:57:23,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33005'
2024-08-09 05:57:23,676 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40665'
2024-08-09 05:57:23,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:37270'
2024-08-09 05:57:23,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40484'
2024-08-09 05:57:23,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39348'
2024-08-09 05:57:23,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39951'
2024-08-09 05:57:24,771 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43080
2024-08-09 05:57:24,771 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43080
2024-08-09 05:57:24,771 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 05:57:24,771 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39707
2024-08-09 05:57:24,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,771 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,771 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,771 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,771 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ycegnv5g
2024-08-09 05:57:24,771 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,772 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40703
2024-08-09 05:57:24,772 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40703
2024-08-09 05:57:24,772 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 05:57:24,772 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42960
2024-08-09 05:57:24,772 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,772 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,772 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,772 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,772 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nu4293q5
2024-08-09 05:57:24,772 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,773 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33927
2024-08-09 05:57:24,773 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33927
2024-08-09 05:57:24,773 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 05:57:24,773 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44851
2024-08-09 05:57:24,773 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,773 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,773 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,773 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-psywp4r1
2024-08-09 05:57:24,774 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,777 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35279
2024-08-09 05:57:24,777 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39417
2024-08-09 05:57:24,777 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35279
2024-08-09 05:57:24,777 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39417
2024-08-09 05:57:24,778 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 05:57:24,778 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 05:57:24,778 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44607
2024-08-09 05:57:24,778 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33161
2024-08-09 05:57:24,778 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,778 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,778 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,778 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,778 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,778 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pkgspciy
2024-08-09 05:57:24,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p7rwioig
2024-08-09 05:57:24,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,778 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43521
2024-08-09 05:57:24,778 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43521
2024-08-09 05:57:24,778 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 05:57:24,778 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42784
2024-08-09 05:57:24,779 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,779 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,779 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,779 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,779 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w41t0v7f
2024-08-09 05:57:24,779 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,779 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44056
2024-08-09 05:57:24,780 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44056
2024-08-09 05:57:24,780 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 05:57:24,780 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36748
2024-08-09 05:57:24,780 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46023
2024-08-09 05:57:24,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,781 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,781 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46023
2024-08-09 05:57:24,781 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,781 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,781 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 05:57:24,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5lflkr6v
2024-08-09 05:57:24,781 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43161
2024-08-09 05:57:24,781 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,781 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,781 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,781 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oxra0zw0
2024-08-09 05:57:24,781 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,781 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35410
2024-08-09 05:57:24,781 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35410
2024-08-09 05:57:24,781 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38439
2024-08-09 05:57:24,781 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 05:57:24,781 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38357
2024-08-09 05:57:24,781 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38439
2024-08-09 05:57:24,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,781 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 05:57:24,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,782 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44829
2024-08-09 05:57:24,782 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,782 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-86k1ajwr
2024-08-09 05:57:24,782 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i92v67w_
2024-08-09 05:57:24,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,787 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:34883
2024-08-09 05:57:24,787 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:34883
2024-08-09 05:57:24,787 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 05:57:24,787 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33287
2024-08-09 05:57:24,787 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,787 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,787 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,787 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z3f37oxi
2024-08-09 05:57:24,787 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,789 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43742
2024-08-09 05:57:24,789 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38202
2024-08-09 05:57:24,789 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43742
2024-08-09 05:57:24,789 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38202
2024-08-09 05:57:24,789 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 05:57:24,789 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 05:57:24,789 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45351
2024-08-09 05:57:24,789 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37167
2024-08-09 05:57:24,789 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,790 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,790 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,790 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,790 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,790 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,790 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,790 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6s451n49
2024-08-09 05:57:24,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s837e5uq
2024-08-09 05:57:24,790 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,790 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,796 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42088
2024-08-09 05:57:24,796 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42088
2024-08-09 05:57:24,796 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 05:57:24,796 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36841
2024-08-09 05:57:24,797 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,797 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,797 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,797 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,797 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-774pjn7t
2024-08-09 05:57:24,797 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,801 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43299
2024-08-09 05:57:24,801 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43299
2024-08-09 05:57:24,801 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 05:57:24,801 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45891
2024-08-09 05:57:24,801 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,801 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,801 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,801 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c7o424f0
2024-08-09 05:57:24,801 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,801 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33589
2024-08-09 05:57:24,801 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33589
2024-08-09 05:57:24,801 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 05:57:24,802 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45122
2024-08-09 05:57:24,802 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:46817
2024-08-09 05:57:24,802 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:24,802 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:57:24,802 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:57:24,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7yxbx91a
2024-08-09 05:57:24,802 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,155 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,155 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,156 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,157 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,157 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,157 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,158 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,158 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,158 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,159 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,159 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,160 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,159 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,160 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,160 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,161 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,162 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,162 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,173 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,174 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,174 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,174 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,174 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,174 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,175 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,175 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,175 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,176 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,178 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,178 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,179 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,179 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,179 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,180 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,180 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,181 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,181 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,181 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,182 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,182 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,182 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,183 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,183 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,183 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,183 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,184 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,184 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
2024-08-09 05:57:25,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:57:25,190 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:46817
2024-08-09 05:57:25,191 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:57:25,191 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:46817
slurmstepd: error: *** JOB 2330381 ON cl-node002 CANCELLED AT 2024-08-09T05:57:36 ***
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39417. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44056. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46023. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43521. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:34883. Reason: scheduler-close
2024-08-09 05:57:36,941 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33927. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38439. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33589. Reason: scheduler-close
2024-08-09 05:57:36,941 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35279. Reason: scheduler-close
2024-08-09 05:57:36,941 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38202. Reason: scheduler-close
2024-08-09 05:57:36,941 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35410. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42088. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43742. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40703. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43080. Reason: scheduler-close
2024-08-09 05:57:36,940 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43299. Reason: scheduler-close
2024-08-09 05:57:36,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48514 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48514 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48504 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48504 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48512 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48512 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48516 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48516 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48500 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48500 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,942 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48524 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48524 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48520 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48520 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48496 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48496 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,943 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48510 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48510 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,944 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48518 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48518 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,944 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48494 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48494 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,946 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37752'. Reason: scheduler-close
2024-08-09 05:57:36,944 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48522 remote=tcp://10.34.59.1:46817>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:48522 remote=tcp://10.34.59.1:46817>: Stream is closed
2024-08-09 05:57:36,949 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36034'. Reason: scheduler-close
2024-08-09 05:57:36,949 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,949 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,949 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35348'. Reason: scheduler-close
2024-08-09 05:57:36,949 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39348'. Reason: scheduler-close
2024-08-09 05:57:36,951 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,951 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,951 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,951 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,951 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,952 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,954 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43223'. Reason: scheduler-close
2024-08-09 05:57:36,955 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34869'. Reason: scheduler-close
2024-08-09 05:57:36,955 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33005'. Reason: scheduler-close
2024-08-09 05:57:36,955 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42068'. Reason: scheduler-close
2024-08-09 05:57:36,956 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35969'. Reason: scheduler-close
2024-08-09 05:57:36,956 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40484'. Reason: scheduler-close
2024-08-09 05:57:36,956 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,956 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40665'. Reason: scheduler-close
2024-08-09 05:57:36,956 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,957 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:37270'. Reason: scheduler-close
2024-08-09 05:57:36,957 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,957 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,957 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,957 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41539'. Reason: scheduler-close
2024-08-09 05:57:36,957 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,957 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44384'. Reason: scheduler-close
2024-08-09 05:57:36,958 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39140'. Reason: scheduler-close
2024-08-09 05:57:36,958 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39951'. Reason: scheduler-close
2024-08-09 05:57:36,958 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,959 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,959 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,959 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,959 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,959 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,959 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,959 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,960 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,960 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,960 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,960 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,960 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,960 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,961 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,961 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:36,961 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:46817; closing.
2024-08-09 05:57:36,962 - distributed.nanny - INFO - Worker closed
