2024-08-09 04:02:06,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46018'
2024-08-09 04:02:06,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41699'
2024-08-09 04:02:06,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40705'
2024-08-09 04:02:06,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41436'
2024-08-09 04:02:06,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36061'
2024-08-09 04:02:06,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41060'
2024-08-09 04:02:06,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38232'
2024-08-09 04:02:06,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:32999'
2024-08-09 04:02:06,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33279'
2024-08-09 04:02:06,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38697'
2024-08-09 04:02:06,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39200'
2024-08-09 04:02:06,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33818'
2024-08-09 04:02:06,652 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45437'
2024-08-09 04:02:06,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33080'
2024-08-09 04:02:06,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34541'
2024-08-09 04:02:06,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41115'
2024-08-09 04:02:07,763 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40186
2024-08-09 04:02:07,763 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40186
2024-08-09 04:02:07,763 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39302
2024-08-09 04:02:07,763 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 04:02:07,763 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39302
2024-08-09 04:02:07,763 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46823
2024-08-09 04:02:07,763 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 04:02:07,763 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,763 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44305
2024-08-09 04:02:07,763 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,763 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,763 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,763 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42419
2024-08-09 04:02:07,763 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ffpfbnp
2024-08-09 04:02:07,763 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,763 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42419
2024-08-09 04:02:07,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8rci92j2
2024-08-09 04:02:07,763 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 04:02:07,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,763 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36569
2024-08-09 04:02:07,763 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,763 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,763 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jw2x0xl3
2024-08-09 04:02:07,763 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45422
2024-08-09 04:02:07,764 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45422
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40024
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45528
2024-08-09 04:02:07,764 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:42960
2024-08-09 04:02:07,764 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36934
2024-08-09 04:02:07,764 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40024
2024-08-09 04:02:07,764 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45528
2024-08-09 04:02:07,764 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,764 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 04:02:07,764 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 04:02:07,764 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:42960
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44267
2024-08-09 04:02:07,764 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40751
2024-08-09 04:02:07,764 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 04:02:07,764 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,764 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,764 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,764 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37932
2024-08-09 04:02:07,764 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fsky484d
2024-08-09 04:02:07,764 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,764 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,764 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q683e37h
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_axymqni
2024-08-09 04:02:07,764 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46133
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ynswd04y
2024-08-09 04:02:07,764 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,764 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46133
2024-08-09 04:02:07,765 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45575
2024-08-09 04:02:07,765 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,765 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 04:02:07,765 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45575
2024-08-09 04:02:07,765 - distributed.worker - INFO -          dashboard at:           10.34.59.2:37578
2024-08-09 04:02:07,765 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 04:02:07,765 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,765 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39949
2024-08-09 04:02:07,765 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,765 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,765 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,765 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,766 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,766 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,766 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kvyvx5is
2024-08-09 04:02:07,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o1uzo3hw
2024-08-09 04:02:07,766 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,766 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,766 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40157
2024-08-09 04:02:07,766 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40157
2024-08-09 04:02:07,766 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 04:02:07,767 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40037
2024-08-09 04:02:07,767 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,767 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38033
2024-08-09 04:02:07,768 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,768 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38033
2024-08-09 04:02:07,768 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 04:02:07,768 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,768 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41665
2024-08-09 04:02:07,768 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,768 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,768 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dbx8p91y
2024-08-09 04:02:07,768 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,768 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,769 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,769 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,769 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oqtefxku
2024-08-09 04:02:07,769 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,770 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41596
2024-08-09 04:02:07,770 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41596
2024-08-09 04:02:07,770 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 04:02:07,770 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36899
2024-08-09 04:02:07,771 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,771 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,771 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,771 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,771 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z0i_uty0
2024-08-09 04:02:07,771 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,772 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37418
2024-08-09 04:02:07,772 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37418
2024-08-09 04:02:07,772 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 04:02:07,772 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34366
2024-08-09 04:02:07,773 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,773 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,773 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,773 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kd92cxoj
2024-08-09 04:02:07,773 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,773 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43145
2024-08-09 04:02:07,774 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:40930
2024-08-09 04:02:07,774 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43145
2024-08-09 04:02:07,774 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 04:02:07,774 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:40930
2024-08-09 04:02:07,774 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45844
2024-08-09 04:02:07,774 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 04:02:07,774 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,774 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,774 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43198
2024-08-09 04:02:07,774 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,774 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,774 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,774 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,774 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7mb68q_h
2024-08-09 04:02:07,774 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,774 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,774 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_kdq4jfx
2024-08-09 04:02:07,774 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,778 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38769
2024-08-09 04:02:07,778 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38769
2024-08-09 04:02:07,778 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 04:02:07,778 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43014
2024-08-09 04:02:07,778 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:37757
2024-08-09 04:02:07,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:07,778 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:02:07,778 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:02:07,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-npnjvexi
2024-08-09 04:02:07,778 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,145 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,145 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,146 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,146 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,146 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,146 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,147 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,147 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,147 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,147 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,147 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,148 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,148 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,149 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,149 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,149 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,150 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,150 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,150 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,151 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,151 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,151 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,151 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,152 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,152 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,152 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,154 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,154 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,154 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,158 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,158 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,158 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,162 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,162 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,163 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,163 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,167 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,167 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,167 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,169 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,169 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,169 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,170 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,171 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,171 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,171 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,172 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,172 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,173 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,173 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,173 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,174 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
2024-08-09 04:02:08,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:02:08,175 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:37757
2024-08-09 04:02:08,176 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:02:08,176 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:37757
slurmstepd: error: *** JOB 2328830 ON cl-node002 CANCELLED AT 2024-08-09T04:02:16 ***
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40186. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45575. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40930. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38033. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45422. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38769. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43145. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39302. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42960. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45528. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46133. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37418. Reason: scheduler-close
2024-08-09 04:02:16,523 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40157. Reason: scheduler-close
2024-08-09 04:02:16,524 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:40024. Reason: scheduler-close
2024-08-09 04:02:16,524 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41596. Reason: scheduler-close
2024-08-09 04:02:16,524 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:42419. Reason: scheduler-close
2024-08-09 04:02:16,525 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53486 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53486 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,525 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53478 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53478 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,525 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53464 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53464 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53460 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53460 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53458 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53458 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,525 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53482 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53482 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53466 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53466 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53480 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53480 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53474 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53474 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53470 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53470 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,526 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53468 remote=tcp://10.34.59.1:37757>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:53468 remote=tcp://10.34.59.1:37757>: Stream is closed
2024-08-09 04:02:16,530 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41699'. Reason: scheduler-close
2024-08-09 04:02:16,531 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33080'. Reason: scheduler-close
2024-08-09 04:02:16,531 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36061'. Reason: scheduler-close
2024-08-09 04:02:16,531 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41436'. Reason: scheduler-close
2024-08-09 04:02:16,533 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,533 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,533 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,533 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,533 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,533 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,533 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45437'. Reason: scheduler-close
2024-08-09 04:02:16,534 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,534 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,535 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,535 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,538 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46018'. Reason: scheduler-close
2024-08-09 04:02:16,538 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34541'. Reason: scheduler-close
2024-08-09 04:02:16,538 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38232'. Reason: scheduler-close
2024-08-09 04:02:16,539 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33818'. Reason: scheduler-close
2024-08-09 04:02:16,539 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:32999'. Reason: scheduler-close
2024-08-09 04:02:16,539 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41060'. Reason: scheduler-close
2024-08-09 04:02:16,539 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,540 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,540 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40705'. Reason: scheduler-close
2024-08-09 04:02:16,540 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,540 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,540 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41115'. Reason: scheduler-close
2024-08-09 04:02:16,540 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,540 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,540 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,540 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39200'. Reason: scheduler-close
2024-08-09 04:02:16,541 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,541 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38697'. Reason: scheduler-close
2024-08-09 04:02:16,541 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,541 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,541 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33279'. Reason: scheduler-close
2024-08-09 04:02:16,542 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,542 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,542 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,542 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,543 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,544 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,544 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,544 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,544 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,544 - distributed.nanny - INFO - Worker closed
2024-08-09 04:02:16,544 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:37757; closing.
2024-08-09 04:02:16,545 - distributed.nanny - INFO - Worker closed
