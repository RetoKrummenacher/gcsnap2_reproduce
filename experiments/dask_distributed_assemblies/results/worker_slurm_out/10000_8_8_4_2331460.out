2024-08-09 08:18:47,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:40083'
2024-08-09 08:18:47,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:39856'
2024-08-09 08:18:47,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:38781'
2024-08-09 08:18:47,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:39879'
2024-08-09 08:18:47,684 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:38173'
2024-08-09 08:18:47,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:38602'
2024-08-09 08:18:47,692 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:45155'
2024-08-09 08:18:47,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.10:40714'
2024-08-09 08:18:48,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dr8x03q1', purging
2024-08-09 08:18:48,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nz3tdc4d', purging
2024-08-09 08:18:48,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-keugipdo', purging
2024-08-09 08:18:48,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-11mtumux', purging
2024-08-09 08:18:48,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-c2259xee', purging
2024-08-09 08:18:48,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-d_bsikaf', purging
2024-08-09 08:18:48,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5v9zr22j', purging
2024-08-09 08:18:48,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-j53uive2', purging
2024-08-09 08:18:48,781 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:46085
2024-08-09 08:18:48,781 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:46085
2024-08-09 08:18:48,781 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-3
2024-08-09 08:18:48,781 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:46836
2024-08-09 08:18:48,781 - distributed.worker - INFO -          dashboard at:          10.34.59.10:46177
2024-08-09 08:18:48,781 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:46836
2024-08-09 08:18:48,781 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,781 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:34498
2024-08-09 08:18:48,781 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,781 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-5
2024-08-09 08:18:48,781 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:34498
2024-08-09 08:18:48,781 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,782 - distributed.worker - INFO -          dashboard at:          10.34.59.10:45892
2024-08-09 08:18:48,782 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-6
2024-08-09 08:18:48,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,782 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,782 - distributed.worker - INFO -          dashboard at:          10.34.59.10:38852
2024-08-09 08:18:48,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-09g8wxdy
2024-08-09 08:18:48,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,782 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,782 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,782 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jqt89krp
2024-08-09 08:18:48,782 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qvso6lui
2024-08-09 08:18:48,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,782 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,786 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:46358
2024-08-09 08:18:48,786 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:44772
2024-08-09 08:18:48,786 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:46358
2024-08-09 08:18:48,786 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:44772
2024-08-09 08:18:48,786 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-1
2024-08-09 08:18:48,786 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-2
2024-08-09 08:18:48,786 - distributed.worker - INFO -          dashboard at:          10.34.59.10:44793
2024-08-09 08:18:48,786 - distributed.worker - INFO -          dashboard at:          10.34.59.10:42667
2024-08-09 08:18:48,786 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,786 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,786 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,786 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,786 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,786 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,786 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,786 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6zvabqh7
2024-08-09 08:18:48,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5lb8zu5v
2024-08-09 08:18:48,786 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,786 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,807 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:37421
2024-08-09 08:18:48,807 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:37421
2024-08-09 08:18:48,807 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-4
2024-08-09 08:18:48,807 - distributed.worker - INFO -          dashboard at:          10.34.59.10:40286
2024-08-09 08:18:48,807 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,807 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,807 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,807 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,807 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_4rsumj
2024-08-09 08:18:48,808 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,810 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:36969
2024-08-09 08:18:48,810 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:36969
2024-08-09 08:18:48,810 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-7
2024-08-09 08:18:48,810 - distributed.worker - INFO -          dashboard at:          10.34.59.10:35871
2024-08-09 08:18:48,810 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,810 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,810 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,810 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,810 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mon3_pbo
2024-08-09 08:18:48,810 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,810 - distributed.worker - INFO -       Start worker at:    tcp://10.34.59.10:43975
2024-08-09 08:18:48,811 - distributed.worker - INFO -          Listening to:    tcp://10.34.59.10:43975
2024-08-09 08:18:48,811 - distributed.worker - INFO -           Worker name:           SLURMCluster-3-0
2024-08-09 08:18:48,811 - distributed.worker - INFO -          dashboard at:          10.34.59.10:38820
2024-08-09 08:18:48,811 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:41225
2024-08-09 08:18:48,811 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:48,811 - distributed.worker - INFO -               Threads:                          1
2024-08-09 08:18:48,811 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 08:18:48,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tqe5v5g2
2024-08-09 08:18:48,811 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,153 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,153 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,154 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,154 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,154 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,155 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,155 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,155 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,155 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,156 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,156 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,156 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,157 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,157 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,157 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,173 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,173 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,173 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,173 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,174 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,174 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
2024-08-09 08:18:49,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 08:18:49,190 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:41225
2024-08-09 08:18:49,190 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 08:18:49,190 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:41225
slurmstepd: error: *** JOB 2331460 ON cl-node010 CANCELLED AT 2024-08-09T08:18:59 ***
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:46085. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:46836. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:34498. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:44772. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:46358. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:37421. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:36969. Reason: scheduler-close
2024-08-09 08:18:59,172 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.10:43975. Reason: scheduler-close
2024-08-09 08:18:59,173 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56440 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56440 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,173 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56438 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56438 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,173 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56442 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56442 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,174 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56445 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56445 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,174 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56452 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56452 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,174 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56444 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56444 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,174 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56448 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56448 remote=tcp://10.34.59.1:41225>: Stream is closed
2024-08-09 08:18:59,175 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56450 remote=tcp://10.34.59.1:41225>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.10:56450 remote=tcp://10.34.59.1:41225>: Stream is closed
