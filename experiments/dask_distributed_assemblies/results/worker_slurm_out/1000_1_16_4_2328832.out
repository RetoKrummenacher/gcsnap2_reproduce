2024-08-09 04:03:06,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46216'
2024-08-09 04:03:06,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33343'
2024-08-09 04:03:06,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35941'
2024-08-09 04:03:06,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41162'
2024-08-09 04:03:06,678 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33006'
2024-08-09 04:03:06,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34961'
2024-08-09 04:03:06,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33298'
2024-08-09 04:03:06,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:44421'
2024-08-09 04:03:06,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34368'
2024-08-09 04:03:06,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:42155'
2024-08-09 04:03:06,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46741'
2024-08-09 04:03:06,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:32886'
2024-08-09 04:03:06,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35211'
2024-08-09 04:03:06,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40900'
2024-08-09 04:03:06,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:45836'
2024-08-09 04:03:06,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39947'
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43471
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35917
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35393
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44754
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43471
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35917
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35393
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45746
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44754
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36322
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45490
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38568
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41923
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34450
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45746
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37810
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45110
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:33630
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36322
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33308
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46769
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45490
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46600
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38568
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41923
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35295
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37810
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33308
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46769
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41212
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36234
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38739
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45346
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35295
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42339
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40742
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41212
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33072
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36630
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36234
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42292
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 04:03:07,917 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41404
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45504
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33072
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xrjqbwwy
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u7tt1vhz
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p3nxigca
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:41256
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:34746
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_az7i8h4
2024-08-09 04:03:07,917 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40976
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,917 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,917 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,917 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,918 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,918 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43524
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i8aqfjvc
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mho7ve6q
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cj44mh_r
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:42844
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l8rs778m
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o0zztn6q
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6mgv0stz
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qh_t58g0
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9re7_iat
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -               Threads:                          1
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6e11h2bv
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zc06eufg
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q018rk6n
2024-08-09 04:03:07,918 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7qstwzuo
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:07,918 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,359 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,359 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,360 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,360 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,360 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,360 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,361 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,361 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,361 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,361 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,361 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,362 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,362 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,363 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,363 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,363 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,363 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,364 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,364 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,364 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,365 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,365 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,366 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,367 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,367 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,367 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,373 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,374 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,374 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,375 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,375 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,375 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,376 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,376 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,376 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,377 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,377 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,377 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,377 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,378 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,378 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,378 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,379 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,379 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,379 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,379 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,379 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,380 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:08,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 04:03:08,381 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:42844
2024-08-09 04:03:08,381 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 04:03:08,381 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:42844
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43471. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41212. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35295. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36234. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38568. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33308. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41923. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36322. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37810. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33072. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46769. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45490. Reason: scheduler-close
2024-08-09 04:03:16,673 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35393. Reason: scheduler-close
slurmstepd: error: *** JOB 2328832 ON cl-node002 CANCELLED AT 2024-08-09T04:03:16 ***
2024-08-09 04:03:16,674 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44754. Reason: scheduler-close
2024-08-09 04:03:16,674 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35917. Reason: scheduler-close
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41144 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41144 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,674 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45746. Reason: scheduler-close
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41128 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41128 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41118 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41118 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41134 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41134 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41132 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41132 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41140 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41140 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41138 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41138 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41148 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41148 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41142 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41142 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41130 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41130 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,676 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41122 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41122 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,677 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41124 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41124 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41146 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41146 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,677 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41120 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41120 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,675 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41136 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41136 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,677 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41126 remote=tcp://10.34.59.1:42844>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:41126 remote=tcp://10.34.59.1:42844>: Stream is closed
2024-08-09 04:03:16,680 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39947'. Reason: scheduler-close
2024-08-09 04:03:16,680 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33298'. Reason: scheduler-close
2024-08-09 04:03:16,681 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:32886'. Reason: scheduler-close
2024-08-09 04:03:16,681 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:44421'. Reason: scheduler-close
2024-08-09 04:03:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35941'. Reason: scheduler-close
2024-08-09 04:03:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34961'. Reason: scheduler-close
2024-08-09 04:03:16,682 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,682 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,682 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33006'. Reason: scheduler-close
2024-08-09 04:03:16,683 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,683 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,683 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,683 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,683 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,683 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,684 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,684 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,684 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,684 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,685 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,685 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,685 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35211'. Reason: scheduler-close
2024-08-09 04:03:16,685 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33343'. Reason: scheduler-close
2024-08-09 04:03:16,686 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:45836'. Reason: scheduler-close
2024-08-09 04:03:16,686 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34368'. Reason: scheduler-close
2024-08-09 04:03:16,686 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46216'. Reason: scheduler-close
2024-08-09 04:03:16,688 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,688 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,688 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,688 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,688 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,688 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,688 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46741'. Reason: scheduler-close
2024-08-09 04:03:16,689 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,689 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40900'. Reason: scheduler-close
2024-08-09 04:03:16,689 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,689 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41162'. Reason: scheduler-close
2024-08-09 04:03:16,689 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,689 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,690 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:42155'. Reason: scheduler-close
2024-08-09 04:03:16,690 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,691 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,691 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,691 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,691 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,692 - distributed.nanny - INFO - Worker closed
2024-08-09 04:03:16,692 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:42844; closing.
2024-08-09 04:03:16,692 - distributed.nanny - INFO - Worker closed
