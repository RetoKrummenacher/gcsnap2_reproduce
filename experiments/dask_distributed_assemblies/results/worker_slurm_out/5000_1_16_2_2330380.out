2024-08-09 05:56:57,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36112'
2024-08-09 05:56:57,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33650'
2024-08-09 05:56:57,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:46597'
2024-08-09 05:56:57,571 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:34267'
2024-08-09 05:56:57,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:33161'
2024-08-09 05:56:57,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38526'
2024-08-09 05:56:57,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:39896'
2024-08-09 05:56:57,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38540'
2024-08-09 05:56:57,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:40603'
2024-08-09 05:56:57,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:36383'
2024-08-09 05:56:57,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:41968'
2024-08-09 05:56:57,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:43138'
2024-08-09 05:56:57,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38600'
2024-08-09 05:56:57,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38261'
2024-08-09 05:56:57,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:35374'
2024-08-09 05:56:57,598 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.34.59.2:38896'
2024-08-09 05:56:58,190 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-lzije4mr', purging
2024-08-09 05:56:58,685 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35970
2024-08-09 05:56:58,685 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35970
2024-08-09 05:56:58,685 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2024-08-09 05:56:58,685 - distributed.worker - INFO -          dashboard at:           10.34.59.2:45638
2024-08-09 05:56:58,685 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,685 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,685 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,685 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j960i_xp
2024-08-09 05:56:58,685 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,689 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44626
2024-08-09 05:56:58,689 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39737
2024-08-09 05:56:58,689 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44626
2024-08-09 05:56:58,689 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39737
2024-08-09 05:56:58,689 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2024-08-09 05:56:58,689 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-4
2024-08-09 05:56:58,689 - distributed.worker - INFO -          dashboard at:           10.34.59.2:43790
2024-08-09 05:56:58,689 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39241
2024-08-09 05:56:58,689 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,689 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,689 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,689 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,689 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,689 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,689 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,689 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2oavvwbd
2024-08-09 05:56:58,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1l3ixvvk
2024-08-09 05:56:58,689 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,689 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,690 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:46583
2024-08-09 05:56:58,690 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:46583
2024-08-09 05:56:58,690 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2024-08-09 05:56:58,690 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36384
2024-08-09 05:56:58,690 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,690 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,690 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,690 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,690 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-75ou0qwv
2024-08-09 05:56:58,690 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,693 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:36938
2024-08-09 05:56:58,693 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:36938
2024-08-09 05:56:58,693 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2024-08-09 05:56:58,693 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44226
2024-08-09 05:56:58,693 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,693 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,693 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,693 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3dlzr1x4
2024-08-09 05:56:58,694 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,694 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35387
2024-08-09 05:56:58,694 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35387
2024-08-09 05:56:58,694 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-6
2024-08-09 05:56:58,694 - distributed.worker - INFO -          dashboard at:           10.34.59.2:40834
2024-08-09 05:56:58,694 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,694 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,695 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,695 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nyzmce40
2024-08-09 05:56:58,695 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,699 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:39039
2024-08-09 05:56:58,699 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:39039
2024-08-09 05:56:58,699 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-7
2024-08-09 05:56:58,699 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38939
2024-08-09 05:56:58,699 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,699 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,699 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,699 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hav_s_r4
2024-08-09 05:56:58,699 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,701 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:44052
2024-08-09 05:56:58,701 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33716
2024-08-09 05:56:58,701 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:44052
2024-08-09 05:56:58,702 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-5
2024-08-09 05:56:58,702 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33716
2024-08-09 05:56:58,702 - distributed.worker - INFO -          dashboard at:           10.34.59.2:35049
2024-08-09 05:56:58,702 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,702 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-8
2024-08-09 05:56:58,702 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,702 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36420
2024-08-09 05:56:58,702 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,702 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,702 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,702 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ka7c4h64
2024-08-09 05:56:58,702 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,702 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,702 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2oh81cpj
2024-08-09 05:56:58,702 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,703 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:35731
2024-08-09 05:56:58,704 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:35731
2024-08-09 05:56:58,704 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-9
2024-08-09 05:56:58,704 - distributed.worker - INFO -          dashboard at:           10.34.59.2:36532
2024-08-09 05:56:58,704 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,704 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,704 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,704 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,704 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b4qs_l6g
2024-08-09 05:56:58,704 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,709 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:43088
2024-08-09 05:56:58,710 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:43088
2024-08-09 05:56:58,710 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-10
2024-08-09 05:56:58,710 - distributed.worker - INFO -          dashboard at:           10.34.59.2:44881
2024-08-09 05:56:58,710 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,710 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,710 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,710 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,710 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sphap2h6
2024-08-09 05:56:58,710 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,710 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:41795
2024-08-09 05:56:58,710 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:41795
2024-08-09 05:56:58,711 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-13
2024-08-09 05:56:58,711 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39591
2024-08-09 05:56:58,711 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,711 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,711 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,711 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zxfifbst
2024-08-09 05:56:58,711 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,712 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:38322
2024-08-09 05:56:58,712 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:38322
2024-08-09 05:56:58,712 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-11
2024-08-09 05:56:58,712 - distributed.worker - INFO -          dashboard at:           10.34.59.2:46154
2024-08-09 05:56:58,713 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,713 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,713 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,713 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,713 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-li910biy
2024-08-09 05:56:58,713 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,716 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:45589
2024-08-09 05:56:58,716 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:37470
2024-08-09 05:56:58,717 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:45589
2024-08-09 05:56:58,717 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:37470
2024-08-09 05:56:58,717 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-12
2024-08-09 05:56:58,717 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-15
2024-08-09 05:56:58,717 - distributed.worker - INFO -          dashboard at:           10.34.59.2:42323
2024-08-09 05:56:58,717 - distributed.worker - INFO -          dashboard at:           10.34.59.2:38371
2024-08-09 05:56:58,717 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,717 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,717 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,717 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,717 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,717 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,717 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,717 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,717 - distributed.worker - INFO -       Start worker at:     tcp://10.34.59.2:33954
2024-08-09 05:56:58,717 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2s2p1jgf
2024-08-09 05:56:58,717 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tg0324x2
2024-08-09 05:56:58,717 - distributed.worker - INFO -          Listening to:     tcp://10.34.59.2:33954
2024-08-09 05:56:58,717 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,717 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,717 - distributed.worker - INFO -           Worker name:          SLURMCluster-0-14
2024-08-09 05:56:58,717 - distributed.worker - INFO -          dashboard at:           10.34.59.2:39511
2024-08-09 05:56:58,717 - distributed.worker - INFO - Waiting to connect to:     tcp://10.34.59.1:33915
2024-08-09 05:56:58,717 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:58,717 - distributed.worker - INFO -               Threads:                          1
2024-08-09 05:56:58,717 - distributed.worker - INFO -                Memory:                   1.86 GiB
2024-08-09 05:56:58,717 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-be463y10
2024-08-09 05:56:58,717 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,065 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,066 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,066 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,066 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,067 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,067 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,067 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,068 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,068 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,068 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,069 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,069 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,069 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,069 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,070 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,070 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,079 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,080 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,080 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,080 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,081 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,081 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,082 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,082 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,082 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,083 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,083 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,084 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,085 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,085 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,085 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,086 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,086 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,086 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,089 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,090 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,090 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,095 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,095 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,095 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,095 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,096 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,096 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,096 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,099 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,099 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,099 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
2024-08-09 05:56:59,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-08-09 05:56:59,103 - distributed.worker - INFO -         Registered to:     tcp://10.34.59.1:33915
2024-08-09 05:56:59,103 - distributed.worker - INFO - -------------------------------------------------
2024-08-09 05:56:59,103 - distributed.core - INFO - Starting established connection to tcp://10.34.59.1:33915
slurmstepd: error: *** JOB 2330380 ON cl-node002 CANCELLED AT 2024-08-09T05:57:10 ***
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:46583. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:36938. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35731. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35387. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44626. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:35970. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:45589. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:43088. Reason: scheduler-close
2024-08-09 05:57:10,898 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:44052. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:41795. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:38322. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33954. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:37470. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39737. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:39039. Reason: scheduler-close
2024-08-09 05:57:10,899 - distributed.worker - INFO - Stopping worker at tcp://10.34.59.2:33716. Reason: scheduler-close
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52738 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52738 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52750 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52750 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52754 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52754 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52752 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52752 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52740 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52740 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52735 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52735 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52732 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52732 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52756 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52756 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52742 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52742 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52746 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52746 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,901 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52759 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52759 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,902 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52744 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52744 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,901 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52758 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52758 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,902 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52734 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52734 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,902 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52748 remote=tcp://10.34.59.1:33915>
Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/users/stud/k/kruret00/.local/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://10.34.59.2:52748 remote=tcp://10.34.59.1:33915>: Stream is closed
2024-08-09 05:57:10,905 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:35374'. Reason: scheduler-close
2024-08-09 05:57:10,906 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36383'. Reason: scheduler-close
2024-08-09 05:57:10,906 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:34267'. Reason: scheduler-close
2024-08-09 05:57:10,907 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38261'. Reason: scheduler-close
2024-08-09 05:57:10,907 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:41968'. Reason: scheduler-close
2024-08-09 05:57:10,907 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:39896'. Reason: scheduler-close
2024-08-09 05:57:10,908 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:46597'. Reason: scheduler-close
2024-08-09 05:57:10,908 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,908 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33650'. Reason: scheduler-close
2024-08-09 05:57:10,908 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,908 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,908 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,908 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:43138'. Reason: scheduler-close
2024-08-09 05:57:10,909 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38526'. Reason: scheduler-close
2024-08-09 05:57:10,909 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,909 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,909 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,909 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38600'. Reason: scheduler-close
2024-08-09 05:57:10,909 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,909 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,909 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,910 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,910 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,910 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,910 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,911 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38896'. Reason: scheduler-close
2024-08-09 05:57:10,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,911 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,911 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,911 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,911 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,912 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,913 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,913 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:38540'. Reason: scheduler-close
2024-08-09 05:57:10,913 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:36112'. Reason: scheduler-close
2024-08-09 05:57:10,913 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:33161'. Reason: scheduler-close
2024-08-09 05:57:10,914 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://10.34.59.2:40603'. Reason: scheduler-close
2024-08-09 05:57:10,914 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,914 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,915 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,915 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,915 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,915 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,916 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,916 - distributed.nanny - INFO - Worker closed
2024-08-09 05:57:10,917 - distributed.core - INFO - Received 'close-stream' from tcp://10.34.59.1:33915; closing.
2024-08-09 05:57:10,917 - distributed.nanny - INFO - Worker closed
